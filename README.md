# AI + Quantum Research Platform

Production-style backend for autonomous research orchestration (AI + Quantum), with:
- FastAPI API layer
- multi-phase agent workflow
- persistent ResearchState in SQLite
- structured logging
- strict validation and safety guardrails
- optional Hugging Face Qwen 7B integration
- asynchronous background workflow execution for heavy phases
- history-aware chat assistant endpoint with scoped retrieval
- VS Code extension local execution mode (agent emits local file/command actions; extension runs them)

## 1. Project features

- End-to-end research lifecycle:
  - clarification
  - planning
  - environment preparation
  - dataset generation/acquisition
  - code generation (+ optional quantum gate)
  - execution
  - error recovery
  - evaluation
  - report generation
- Security controls:
  - project-path whitelist checks
  - action schema validation
  - subprocess argument sanitization
  - retry caps
- Logging for all major layers:
  - HTTP request/response logs
  - phase start/end logs
  - DB operation logs
  - subprocess run logs
- RL feedback signals:
  - phase validation rewards
  - runtime success/failure rewards
  - user decision reward signals
  - latency, evaluation-quality, and terminal-outcome rewards/penalties
- Observability and persistence:
  - numeric metrics normalized into `experiment_metrics`
  - LLM usage telemetry (tokens, latency)
  - system-level phase latency histograms and failure clustering
- Chat + history collections:
  - test-mode unified collection (`test:unified`)
  - user-scoped collections (`user:<user_id>`)
  - persisted chat history and token accounting per assistant reply
- Hugging Face model support for master LLM provider (`Qwen 7B` family).

## 2. Services and main modules

- API: `src/api/*`
- Workflow runner: `src/graph/runner.py`
- Agents: `src/agents/*`
- Safety/validation/runtime: `src/core/*`
- Persistence: `src/db/*`
- LLM integrations: `src/llm/*`
- Config: `src/config/settings.py`

## 3. Setup guide

### 3.1 Prerequisites

- Python 3.11
- pip or Poetry

### 3.2 Install dependencies

Option A: `requirements.txt`
```bash
pip install -r requirements.txt
```

Option B: Poetry
```bash
poetry install
```

### 3.3 Environment config

Copy:
```bash
cp .env.example .env
```

Minimum `.env` for Hugging Face Qwen:
```env
MASTER_LLM_PROVIDER=huggingface
HF_API_KEY=hf_xxx
HF_INFERENCE_URL=https://router.huggingface.co/v1
HF_MODEL_ID=Qwen/Qwen2.5-7B-Instruct
ALLOW_RULE_BASED_FALLBACK=false

PROJECT_ROOT=./workspace/projects
STATE_DB_PATH=./workspace/state.db
LOG_LEVEL=INFO
```

If you explicitly want local rule-based fallback mode (not autonomous):
```env
MASTER_LLM_PROVIDER=rule_based
ALLOW_RULE_BASED_FALLBACK=true
```

If `research_type=quantum` should be generated by the external `codehub/backend` (RAG + LLM), set:
```env
CODEHUB_USE_FOR_QUANTUM=true
CODEHUB_BACKEND_BASE_URL=http://127.0.0.1:8001
CODEHUB_GENERATE_ENDPOINT=/api/code/generate
CODEHUB_INTERNAL_API_KEY=
CODEHUB_BEARER_TOKEN=
CODEHUB_TIMEOUT=120
```

Auth options for CodeHub:
- `ENABLE_AUTH=false` in `codehub/backend` for local integration without JWT.
- or keep `ENABLE_AUTH=true` and provide either:
  - `CODEHUB_BEARER_TOKEN` (JWT), or
  - `CODEHUB_INTERNAL_API_KEY` (when configured in `codehub/backend`).

Optional adaptive/resilience toggles:
```env
EXPERIMENT_VENV_ENABLED=true
AUTO_CONFIRM_LOW_RISK=true
LOW_RISK_PACKAGES=numpy,pandas,matplotlib,scikit-learn,requests
WORKFLOW_BACKGROUND_ENABLED=true
EXECUTION_MODE=vscode_extension
LOCAL_PYTHON_COMMAND=python
METRICS_TABLE_ENABLED=true
FAILURE_INJECTION_ENABLED=false
FAILURE_INJECTION_RATE=0.0
FAILURE_INJECTION_POINTS=
AUTO_RETRY_ON_LOW_METRIC=true
MIN_PRIMARY_METRIC_FOR_SUCCESS=0.75
CHAT_CONTEXT_LIMIT_DEFAULT=5
CHAT_CONTEXT_LIMIT_MAX=20
CHAT_HISTORY_LIMIT=40
```

### 3.4 Run server

```bash
uvicorn src.api.app:app --reload --host 127.0.0.1 --port 8000
```

Swagger:
- `http://localhost:8000/api/docs`

## 4. API usage and endpoint examples

Detailed extension guide (all endpoints, request/response formats):
- `API_EXTENSION_GUIDE.md`

Authentication:
- No API key is required for local endpoint access in this build.

Base URL: `http://localhost:8000/api/v1`  
Header on all requests:
```http
Content-Type: application/json
```

### 4.1 Start experiment

`POST /research/start`

Request:
```json
{
  "prompt": "Build a hybrid quantum-classical classifier for synthetic data",
  "research_type": "quantum",
  "priority": "normal",
  "tags": ["quantum", "classification"],
  "config_overrides": {
    "random_seed": 42,
    "hardware_target": "cpu",
    "max_epochs": 20
  }
}
```

Response (example):
```json
{
  "success": true,
  "data": {
    "experiment_id": "exp_20260224_abc123",
    "status": "waiting_user",
    "phase": "clarifier",
    "research_type": "quantum",
    "execution_mode": "vscode_extension",
    "pending_questions": {
      "mode": "sequential_dynamic",
      "current_question": {"id":"Q1","topic":"output_format","type":"choice","text":"Do you want .py scripts or .ipynb notebooks?"},
      "questions": [
        {"id":"Q1","topic":"output_format","type":"choice","text":"Do you want .py scripts or .ipynb notebooks?"}
      ],
      "asked_question_ids": [],
      "answered_count": 0
    }
  }
}
```

### 4.2 Answer clarification questions

`POST /research/{experiment_id}/answer`

Sequential mode: submit one answer per request.

Request:
```json
{
  "answers": {
    "Q1": ".py"
  }
}
```

### 4.3 Confirm pending action

`POST /research/{experiment_id}/confirm`

Request:
```json
{
  "action_id": "act_1234abcd",
  "decision": "confirm",
  "reason": "Approve installation for test run",
  "alternative_preference": "",
  "execution_result": {
    "returncode": 0,
    "stdout": "",
    "stderr": "",
    "duration_sec": 2.4,
    "command": ["python", "main.py"],
    "cwd": "C:/.../exp_...",
    "created_files": ["C:/.../main.py"]
  }
}
```

### 4.4 VS Code extension local run contract

When `EXECUTION_MODE=vscode_extension`, backend phases generate `pending_action` payloads in status/log APIs.
Your extension should:
1. read `pending_action.file_operations` and create/update those files locally
2. run `pending_action.commands` in `pending_action.cwd`
3. call `POST /research/{experiment_id}/confirm` with `decision="confirm"` and `execution_result`

This happens across phases (env/dataset/codegen/quantum/subprocess), not only final script execution.

The backend will then continue to evaluation/doc generation using local run outputs.

### 4.5 Status / logs / results / report

- `GET /research/{experiment_id}/status`
- `GET /research/{experiment_id}/logs`
- `GET /research/{experiment_id}/results`
- `GET /research/{experiment_id}/report?format=markdown&download=false`

### 4.6 Files and system endpoints

- `GET /research/{experiment_id}/files`
- `GET /research/{experiment_id}/files/{file_path}`
- `GET /system/health`
- `GET /system/metrics`
- `POST /chat/research`
- `GET /chat/history`

## 5. End-to-end testing script (`test.py`)

Added root script `test.py` that tests the full flow and stores everything in one JSON report (including request bodies, responses, and metrics).

### 5.1 Run

```bash
python test.py
```

Optional:
```bash
python test.py --scenarios standard,quantum,recovery,abort --timeout-sec 600
python test.py --allow-remote-base-url --base-url http://127.0.0.1:8000/api/v1
python test.py --codehub-base-url http://127.0.0.1:8001
python test.py --ai-only --require-rl-feedback
python test.py --ai-only --min-avg-llm-calls 3.0
```

### 5.2 What it does

- Runs multi-scenario flows (`standard`, `quantum`, `retry`, `abort`)
- Calls all API endpoints at least once across selected scenarios
- Stores each request and response (method/path/body/status/headers/latency) in one consolidated JSON file
- Auto-answers clarification questions and auto-confirms pending actions
- Stores detailed agent trace JSON per scenario:
  - clarification questions asked
  - answers sent to agents
  - confirmation actions
  - subprocess commands executed
  - API step-by-step responses
- Captures scenario-level metrics:
  - endpoint coverage
  - latency stats
  - agent-phase validation from logs
  - LLM provider/model used
  - RL/system metrics snapshot
- Enforces local execution by default (`localhost` only unless explicitly overridden)

### 5.3 Output location

By default:
- `workspace/test_outputs/<timestamp>_full_run.json`

## 6. Logging

Structured logging is enabled via `structlog` and includes:
- `app.startup` / `app.shutdown`
- `http.request.start` / `http.request.end` / `http.request.error`
- `phase.start` / `phase.end`
- validation warnings/errors
- DB create/update/log operations
- subprocess start/end/error
- agent start/end logs

Set log level in `.env`:
```env
LOG_LEVEL=INFO
```

## 7. Notes

- Default mode is strict autonomous LLM execution (`ALLOW_RULE_BASED_FALLBACK=false`), so missing HF key/provider will fail fast instead of silently using rules.
- Keep secrets in `.env` only (never commit keys).
- Existing unit tests can still run:
```bash
pytest -q
```


## 8. Post-run analysis (metrics, endpoints, agent behavior, scenarios, latency)

Use `analyze_run.py` to generate a consolidated analysis file from:
- `workspace/state.db` (experiments, logs, RL feedback, metrics snapshots)
- `workspace/test_outputs/*` (endpoint trace JSON created by `test.py`)

### 8.1 Run

```bash
python analyze_run.py
```

Optional experiment filter:

```bash
python analyze_run.py --experiment-id exp_20260224_b1772d
```

Optional custom paths:

```bash
python analyze_run.py \
  --db-path workspace/state.db \
  --test-outputs-root workspace/test_outputs \
  --output-json workspace/analysis/analysis_summary.json \
  --output-md workspace/analysis/analysis_report.md
```

### 8.2 Output files

- JSON summary: `workspace/analysis/analysis_summary.json`
- Markdown report: `workspace/analysis/analysis_report.md`

### 8.3 What is included

- API endpoint coverage and status/error counts
- API latency stats (`X-Process-Time`) with avg/p95/min/max
- Agent/phase behavior from DB logs and phase completion events
- Scenario-level request patterns (clarification, confirmation, abort flows)
- Metrics aggregation from `state_json.metrics` and `experiment_metrics` table
- RL feedback aggregation (avg reward and positive rate by phase/signal)
- Coverage notes when data sources are missing
